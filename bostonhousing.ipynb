{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "xhMmDS79qHGF",
        "outputId": "d63b0cfe-3794-4f10-aa04-ef2aa696e98d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
            "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
            "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
            "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
            "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
            "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
            "\n",
            "        b  lstat  medv  \n",
            "0  396.90   4.98  24.0  \n",
            "1  396.90   9.14  21.6  \n",
            "2  392.83   4.03  34.7  \n",
            "3  394.63   2.94  33.4  \n",
            "4  396.90   5.33  36.2  \n",
            "The attribute that best follows the linear relationship with the output price is: lstat\n",
            "Analytic solution coefficients: [ 2.25328063e+01 -9.28146064e-01  1.08156863e+00  1.40899997e-01\n",
            "  6.81739725e-01 -2.05671827e+00  2.67423017e+00  1.94660717e-02\n",
            " -3.10404426e+00  2.66221764e+00 -2.07678168e+00 -2.06060666e+00\n",
            "  8.49268418e-01 -3.74362713e+00]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'LinearRegressionGradientDescent' object has no attribute 'max_iter'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7a230f41fce1>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mlr_full_batch_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegressionGradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mlr_full_batch_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gradient descent (Full-batch) coefficients (scaled features):\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_full_batch_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-7a230f41fce1>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_with_bias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX_with_bias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mX_with_bias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LinearRegressionGradientDescent' object has no attribute 'max_iter'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "boston_data = pd.read_csv(\"BostonHousing.csv\")\n",
        "print(boston_data.head())\n",
        "\n",
        "X = boston_data.drop(columns=[\"medv\"])\n",
        "y = boston_data[\"medv\"]\n",
        "\n",
        "correlation_coeffs = np.abs(boston_data.corr()[\"medv\"]).drop(\"medv\")\n",
        "best_feature_name = correlation_coeffs.idxmax()\n",
        "\n",
        "print(f\"The attribute that best follows the linear relationship with the output price is: {best_feature_name}\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_with_bias = np.c_[np.ones((X_scaled.shape[0], 1)), X_scaled]\n",
        "\n",
        "analytic_solution = np.linalg.inv(X_with_bias.T.dot(X_with_bias)).dot(X_with_bias.T).dot(y)\n",
        "\n",
        "print(\"Analytic solution coefficients:\", analytic_solution)\n",
        "\n",
        "class LinearRegressionGradientDescent():\n",
        "    def _init_(self, lr=0.0001, max_iter=10000, tolerance=1e-6):\n",
        "        self.lr = lr\n",
        "        self.max_iter = max_iter\n",
        "        self.tolerance = tolerance\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "        self.weights = np.zeros(X_with_bias.shape[1])\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            gradients = -2 * X_with_bias.T.dot(y - X_with_bias.dot(self.weights))\n",
        "            if np.all(np.abs(gradients) < self.tolerance):\n",
        "                break\n",
        "            self.weights -= self.lr * gradients\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "        return X_with_bias.dot(self.weights)\n",
        "\n",
        "lr_full_batch_scaled = LinearRegressionGradientDescent()\n",
        "lr_full_batch_scaled.fit(X_scaled, y)\n",
        "print(\"Gradient descent (Full-batch) coefficients (scaled features):\", lr_full_batch_scaled.weights)\n",
        "\n",
        "class LinearRegressionStochasticGradientDescent():\n",
        "    def _init_(self, lr=0.0001, max_iter=1000, tolerance=1e-6, batch_size=1):\n",
        "        self.lr = lr\n",
        "        self.max_iter = max_iter\n",
        "        self.tolerance = tolerance\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "        self.weights = np.zeros(X_with_bias.shape[1])\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            indices = np.random.permutation(X_with_bias.shape[0])\n",
        "            for start_idx in range(0, X_with_bias.shape[0], self.batch_size):\n",
        "                batch_indices = indices[start_idx:start_idx + self.batch_size]\n",
        "                X_batch = X_with_bias[batch_indices]\n",
        "                y_batch = y[batch_indices]\n",
        "                gradients = -2 * X_batch.T.dot(y_batch - X_batch.dot(self.weights))\n",
        "                if np.all(np.abs(gradients) < self.tolerance):\n",
        "                    break\n",
        "                self.weights -= self.lr * gradients\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "        return X_with_bias.dot(self.weights)\n",
        "\n",
        "lr_stochastic_scaled = LinearRegressionStochasticGradientDescent(lr=0.0001, max_iter=10000, batch_size=1)\n",
        "lr_stochastic_scaled.fit(X_scaled, y)\n",
        "print(\"Gradient descent (Stochastic) coefficients (scaled features):\", lr_stochastic_scaled.weights)"
      ]
    }
  ]
}