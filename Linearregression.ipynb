{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o79YbTEaOHR3",
        "outputId": "62667643-355d-4b27-96ce-a3d0770c85c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analytic Solution:\n",
            "Coefficients: [1.23636364 1.16969697]\n",
            "Sum of Squared Errors: 5.624242424242421\n",
            "R^2 value: 0.952538038613988\n",
            "\n",
            "Full-batch Gradient Descent:\n",
            "Coefficients: (1.170263693076768, 1.2328099487610318)\n",
            "Sum of Squared Errors: 5.624278989977716\n",
            "R^2 value: 0.9525377300423822\n",
            "\n",
            "Stochastic Gradient Descent:\n",
            "Coefficients: (1.2986755729435908, 0.8967040680508923)\n",
            "Sum of Squared Errors: 7.576246971879953\n",
            "R^2 value: 0.9360654263976376\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])\n",
        "\n",
        "X = np.vstack([np.ones(len(x)), x]).T\n",
        "beta_hat_normal_eq = np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "y_hat_normal_eq = X @ beta_hat_normal_eq\n",
        "SSE_normal_eq = np.sum((y - y_hat_normal_eq)**2)\n",
        "R_squared_normal_eq = 1 - SSE_normal_eq / np.sum((y - np.mean(y))**2)\n",
        "\n",
        "print(\"Analytic Solution:\")\n",
        "print(f\"Coefficients: {beta_hat_normal_eq}\")\n",
        "print(f\"Sum of Squared Errors: {SSE_normal_eq}\")\n",
        "print(f\"R^2 value: {R_squared_normal_eq}\")\n",
        "def full_batch_gradient_descent(x, y, lr=0.01, epochs=1000):\n",
        "    m, b = 0.0, 0.0\n",
        "    N = len(y)\n",
        "    for _ in range(epochs):\n",
        "        y_pred = m*x + b\n",
        "        dm = (-2/N) * np.sum(x * (y - y_pred))\n",
        "        db = (-2/N) * np.sum(y - y_pred)\n",
        "        m -= lr * dm\n",
        "        b -= lr * db\n",
        "    return m, b\n",
        "m_gd, b_gd = full_batch_gradient_descent(x, y)\n",
        "y_pred_gd = m_gd*x + b_gd\n",
        "sse_gd = np.sum((y - y_pred_gd)**2)\n",
        "r_squared_gd = 1 - sse_gd / np.sum((y - np.mean(y))**2)\n",
        "\n",
        "print(\"\\nFull-batch Gradient Descent:\")\n",
        "print(f\"Coefficients: {m_gd, b_gd}\")\n",
        "print(f\"Sum of Squared Errors: {sse_gd}\")\n",
        "print(f\"R^2 value: {r_squared_gd}\")\n",
        "def stochastic_gradient_descent(x, y, lr=0.01, epochs=1000):\n",
        "    m, b = 0.0, 0.0\n",
        "    N = len(y)\n",
        "    for _ in range(epochs):\n",
        "        for i in range(N):\n",
        "            y_pred = m*x[i] + b\n",
        "            dm = -2 * x[i] * (y[i] - y_pred)\n",
        "            db = -2 * (y[i] - y_pred)\n",
        "            m -= lr * dm\n",
        "            b -= lr * db\n",
        "    return m, b\n",
        "m_sgd, b_sgd = stochastic_gradient_descent(x, y)\n",
        "y_pred_sgd = m_sgd*x + b_sgd\n",
        "sse_sgd = np.sum((y - y_pred_sgd)**2)\n",
        "r_squared_sgd = 1 - sse_sgd / np.sum((y - np.mean(y))**2)\n",
        "\n",
        "print(\"\\nStochastic Gradient Descent:\")\n",
        "print(f\"Coefficients: {m_sgd, b_sgd}\")\n",
        "print(f\"Sum of Squared Errors: {sse_sgd}\")\n",
        "print(f\"R^2 value: {r_squared_sgd}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7pYh6ZyRo-Xn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}